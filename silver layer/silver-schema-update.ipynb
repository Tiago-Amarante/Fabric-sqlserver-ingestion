{"cells":[{"cell_type":"markdown","source":["# Update Silver Schema\n","\n","The goal of this Notebook is to update the Schema of all silver tables. It means, that it get the bonze tables schema (Lakehouse/Tables/tableschema) , remove all columns that all values are null, blank or single valued. And save them as the Silver tables schema in a table called tableschema_silver (Lakehouse/Tables/tableschema_silver) "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6787d746-f3e4-4302-b89c-1df5d3d4f3c0"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from functools import reduce\n","from pyspark.sql.functions import monotonically_increasing_id, col, count, countDistinct, broadcast\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"0daed764-000e-4f11-8491-9fc798f56906","statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","queued_time":"2024-05-22T18:43:32.2191041Z","session_start_time":"2024-05-22T18:43:32.588656Z","execution_start_time":"2024-05-22T18:43:43.2180173Z","execution_finish_time":"2024-05-22T18:43:45.7550907Z","parent_msg_id":"e1c86a87-87d6-4f6d-9186-f7b44765e6fc"},"text/plain":"StatementMeta(, 0daed764-000e-4f11-8491-9fc798f56906, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e2bba786-1e0a-4a87-8c57-2817859c4795"},{"cell_type":"code","source":["'''\n","The extract_names function get all bronze tables names,\n","remove the listed tables in tables_to__be_excluded list\n","and return another list with all tables that will heve their schema updated \n","'''\n","# If you don't want to updade automatically update some table, insert them here in tables_to_be_excluded\n","tables_to_be_excluded = ['tableschema','tablesschema_silver', 'sys_company', 'sys_usr',]\n","\n","def extract_names(file_info_list,tables_to_be_excluded):\n","\n","    '''\n","    Get all tables in silver_protheus lakehouse Tables folder that don't contains *_silver in the name\n","    and also is not present in tables_to_be_excluded list\n","    '''\n","    \n","    names = []\n","    for file_info in file_info_list:\n","        name = file_info.name\n","        if \"_silver\" not in name and name not in tables_to_be_excluded:\n","            names.append(name)\n","    return names\n","\n","# list all fileInfo objects of tables in silver_protheus/Tables folder\n","file_info_of_tables = mssparkutils.fs.ls('Tables/')\n","\n","# Parse the fileInfo objects returning a list of desired tables to get the schema updeted of \n","tables_list = extract_names(file_info_of_tables,tables_to_be_excluded)\n","tables_list"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"60e13ed9-4d11-4d36-bba1-35a9a03f76cd","statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-30T21:20:37.9068127Z","session_start_time":null,"execution_start_time":"2024-04-30T21:20:50.806466Z","execution_finish_time":"2024-04-30T21:20:51.0814117Z","parent_msg_id":"00216cf4-dbac-440c-aa21-47646218c327"},"text/plain":"StatementMeta(, 60e13ed9-4d11-4d36-bba1-35a9a03f76cd, 4, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"['ak5010',\n 'aov010',\n 'cnf010',\n 'fp0010',\n 'fp1010',\n 'fpa010',\n 'fpg010',\n 'fq4010',\n 'p10010',\n 'sa1010',\n 'sa2010',\n 'sa6010',\n 'sc5010',\n 'sc7010',\n 'scp010',\n 'scr010',\n 'sd1010',\n 'se1010',\n 'se2010',\n 'se5010',\n 'sed010',\n 'sf1010',\n 'st7010',\n 'st9010',\n 'stj010',\n 'stl010',\n 'tqy010']"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3964a6df-cb56-4c94-ae5d-dab451d7f227"},{"cell_type":"code","source":["'''\n","This function get a dataframe and a list of columns to not be removed\n","and than search for all columns in the dataframe that just have a single-value or non-value\n","'''\n","def find_completely_null_and_single_value_columns(df,columns_to_not_be_removed):\n","    # Create an expression list to count non-nulls in each column\n","    exprs = [countDistinct(col(c)).alias(c) for c in df.columns]\n","\n","    # Apply the expressions in a single aggregation to get non-null counts for all columns\n","    non_null_counts = df.agg(*exprs).first()\n","\n","    # Find columns where the count of non-nulls is 0\n","    empty_and_single_value_columns = [c for c in df.columns if non_null_counts[c] <= 1 ]\n","\n","    # Remove from the list the columns that we want to persist in the schema\n","    empty_and_single_value_columns = [item for item in empty_and_single_value_columns if not any(substring in item for substring in columns_to_not_be_removed)]\n","\n","    return empty_and_single_value_columns"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"60e13ed9-4d11-4d36-bba1-35a9a03f76cd","statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-30T21:20:37.907368Z","session_start_time":null,"execution_start_time":"2024-04-30T21:20:51.6535318Z","execution_finish_time":"2024-04-30T21:20:51.9405751Z","parent_msg_id":"9513394d-cf34-4e82-84b2-1c9e00214f5a"},"text/plain":"StatementMeta(, 60e13ed9-4d11-4d36-bba1-35a9a03f76cd, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6aeb8f19-f0f4-44bc-b960-a725f8a9ff26"},{"cell_type":"code","source":["'''\n","This function get the df_tablesschema_silver\n","and than remove all columns listed in columns_to_be_removed for the especified table\n","'''\n","\n","def generate_silver_table_schema(df_tablesschema_silver,table,columns_to_be_removed,idx):\n","\n","    # select from the schema just the rows of the especified table, removing the columns_to_be_removed\n","    df_aux = df_tablesschema_silver \\\n","    .select('TABLE_NAME','COLUMN_NAME','ORDINAL_POSITION','DATA_TYPE') \\\n","    .where(\n","        (~col('COLUMN_NAME').isin(list(columns_to_be_removed))) & \n","        (col('TABLE_NAME')==table.upper())\n","        )\n","\n","    # create a sequential numbers column to keep the Protheus ordenation of the tables\n","    df_silver_table_schema = df_aux.withColumn(\"ORDINAL_POSITION\", monotonically_increasing_id())\n","\n","    # modify the ORDINAL_POSITION column to keep the Protheus ordenation of the tables\n","    df_silver_table_schema = df_silver_table_schema.withColumn(\"ORDINAL_POSITION\", col(\"ORDINAL_POSITION\")+(idx*1000))\n","    return df_silver_table_schema"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"60e13ed9-4d11-4d36-bba1-35a9a03f76cd","statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-30T21:20:37.9079471Z","session_start_time":null,"execution_start_time":"2024-04-30T21:20:52.5121949Z","execution_finish_time":"2024-04-30T21:20:52.7772437Z","parent_msg_id":"e892d78c-4f94-48f8-bd0d-34304b56c181"},"text/plain":"StatementMeta(, 60e13ed9-4d11-4d36-bba1-35a9a03f76cd, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd8ac0a7-8f8e-46cb-976d-d0665329ad74"},{"cell_type":"code","source":["'''\n","This function get some schema dataframe remove the rows for actual table\n","and than insert (union) with the new updated Data\n","at the end the function update the table in df_tablesschema_silver\n","with the data of df_silver_table_schema (related to table)\n","'''\n","def updade_silver_full_tables_schema(df_tablesschema_silver,table,df_silver_table_schema):\n","    \n","    # select all rows that table name are diferrent of table variable\n","    df_aux = df_tablesschema_silver.select('TABLE_NAME','COLUMN_NAME','ORDINAL_POSITION','DATA_TYPE').where(col('TABLE_NAME')!=table.upper())\n","    \n","    # Union in the df_aux the new table data, creating the updated schema\n","    df_full_silver_updated = df_aux.union(df_silver_table_schema)\n","   \n","    return df_full_silver_updated"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"60e13ed9-4d11-4d36-bba1-35a9a03f76cd","statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-30T21:20:37.9084363Z","session_start_time":null,"execution_start_time":"2024-04-30T21:20:53.3293098Z","execution_finish_time":"2024-04-30T21:20:53.5975966Z","parent_msg_id":"bfdf17f9-f2e2-4fcd-bee3-53a21cdfec9f"},"text/plain":"StatementMeta(, 60e13ed9-4d11-4d36-bba1-35a9a03f76cd, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d0a7f8f-1670-40d0-b8d9-aec8d0b9b2dc"},{"cell_type":"code","source":["'''\n","This function get a spark dataframe and save as a delta table.\n","The path and table name are defined inside the function, also the overwrite mode are activate, so be careful!!\n","'''\n","\n","def write_parquet_file(df_full_silver_updated):\n","    \n","    table_name = f'tableschema_silver'\n","    \n","    df_full_silver_updated.write \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .mode(\"overwrite\") \\\n","    .saveAsTable(table_name, mode=\"overwrite\", ifNotExists=True)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"60e13ed9-4d11-4d36-bba1-35a9a03f76cd","statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-30T21:20:37.9089165Z","session_start_time":null,"execution_start_time":"2024-04-30T21:20:54.1316873Z","execution_finish_time":"2024-04-30T21:20:54.4412111Z","parent_msg_id":"f1bbbbbb-133a-4e49-b19b-6fdbc248412d"},"text/plain":"StatementMeta(, 60e13ed9-4d11-4d36-bba1-35a9a03f76cd, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80db63df-1fba-49b3-8a3d-04eff7cabf0d"},{"cell_type":"code","source":["'''\n","This cell is the main logic of the notebook. Here we are doing these steps:\n","- Copy bronze layer Schema\n","- Automatically detect and remove from silver the columns in bronze with non-values or single-values.\n","  Note that a colum with non-values AND single values (e.g. {None,0}) will not be removed because it was't a single-value it was doble-valued (None and 0)\n","- Update recursivaly (inside the \"for in\") the bronze schema with the new silver schema\n","- Modify the original 'ORDINAL_POSITION' in a way that we can use this column to order the columns like in Protheus\n","- Save the updated silver schema as a delta table named tableschema_silver\n","'''\n","\n","df_tablesschema_raw = spark.sql(\"SELECT * FROM silver_protheus.tableschema ORDER BY sequential_number\")\n","\n","df_tablesschema = spark.sql(\"SELECT TABLE_NAME,COLUMN_NAME,ORDINAL_POSITION,DATA_TYPE FROM silver_protheus.tableschema ORDER BY sequential_number\")\n","\n","columns_to_not_be_removed = ['_FILIAL', 'INGESTION_DATE', 'D_E_L_E_T_', 'R_E_C_N_O_','R_E_C_D_E_L_']\n","\n","log_table_succs = list([])\n","print(log_table_succs)\n","log_table_error = list([])\n","output = {}\n","\n","for idx,table in enumerate(tables_list):\n","    try:\n","      # Get the bronze table to analyze the columns to be removed\n","      df = broadcast(spark.sql(f\"SELECT * FROM silver_protheus.{table}\"))\n","\n","      # create a list of columns with non-value and single-value\n","      empty_and_single_value_columns = find_completely_null_and_single_value_columns(df,columns_to_not_be_removed)\n","\n","      columns_to_be_removed = tuple(empty_and_single_value_columns + list(['a', 'b']))\n","      \n","      # Generate the silver schema for the actual table item on loop\n","      df_silver_table_schema = generate_silver_table_schema(df_tablesschema_raw,table,columns_to_be_removed,(idx+1))\n","      \n","      # update the tableschema with the new tables schema generated earlier\n","      df_tablesschema = updade_silver_full_tables_schema(df_tablesschema,table,df_silver_table_schema)\n","      \n","      log_table_succs.append(f'{table}')\n","      print(log_table_succs)\n","    except Exception as e:\n","\n","      log_table_error.append(f'{table}: {e}')\n","\n","df_tablesschema_silver = df_tablesschema.orderBy('ORDINAL_POSITION')\n","# write the full updated silever schema as Delta Table (also parque file associated with it)\n","write_parquet_file(df_tablesschema_silver)\n","\n","\n","output['successes'] = f'Silver schema update for tables in ({log_table_succs})'\n","output['Fail'] = f'Silver schema update for tables in ({log_table_error})'\n","print(output)\n","mssparkutils.notebook.exit(output)  "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"60e13ed9-4d11-4d36-bba1-35a9a03f76cd","statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-30T21:39:15.2932335Z","session_start_time":null,"execution_start_time":"2024-04-30T21:39:15.8723487Z","execution_finish_time":"2024-04-30T21:39:31.1360071Z","parent_msg_id":"bb232da8-8135-4a35-8546-066f3628143f"},"text/plain":"StatementMeta(, 60e13ed9-4d11-4d36-bba1-35a9a03f76cd, 23, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[]\n['ak5010']\n['ak5010', 'aov010']\n['ak5010', 'aov010', 'cnf010']\n{'successes': \"Silver schema update for tables in (['ak5010', 'aov010', 'cnf010'])\", 'Fail': 'Silver schema update for tables in ([])'}\nExitValue: {'successes': \"Silver schema update for tables in (['ak5010', 'aov010', 'cnf010'])\", 'Fail': 'Silver schema update for tables in ([])'}"]}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0bb0bcfe-74d8-4ea7-9a31-1ebeff0407e6"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"944b20f8-2311-4f42-87db-8c900a876754"}],"default_lakehouse":"944b20f8-2311-4f42-87db-8c900a876754","default_lakehouse_name":"silver_protheus","default_lakehouse_workspace_id":"0973c99d-d67e-4ee4-97d9-1a44821f9152"},"environment":{"environmentId":"f4d6e278-58a7-4327-9cc7-76f29dac5bf4","workspaceId":"0973c99d-d67e-4ee4-97d9-1a44821f9152"}}},"nbformat":4,"nbformat_minor":5}