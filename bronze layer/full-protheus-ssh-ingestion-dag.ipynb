{"cells":[{"cell_type":"code","source":["# Impor the necessary libs\n","from pyspark.sql.functions import *\n","from pyspark.sql import SparkSession\n","import sshtunnel\n","import socket\n","import sys\n","import json\n","from pyspark.sql.functions import current_timestamp, date_format, col, from_utc_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, LongType, DoubleType, IntegerType\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T20:07:19.2557898Z","session_start_time":null,"execution_start_time":"2024-08-08T20:07:19.7701786Z","execution_finish_time":"2024-08-08T20:07:20.0303383Z","parent_msg_id":"23e57783-88ef-47fa-9bdc-5775a2c09986"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d2027fc-9398-483e-8ac0-770a62a0c8b8"},{"cell_type":"code","source":["def cast_numeric_columns_to_common_type(df, target_type=DoubleType()):\n","    '''\n","    This function casts all numeric columns in the given DataFrame to a specified common data type.\n","    \n","    By default, the target type is DoubleType, but it can be changed by providing a different target_type.\n","    \n","    The function performs the following steps:\n","    1. Iterates through each column in the DataFrame schema.\n","    2. Checks if the column's data type is one of the numeric types (IntegerType, LongType, or DoubleType).\n","    3. If the column is numeric, it casts the column to the specified target type.\n","    4. Returns the DataFrame with the updated column types.\n","\n","    Args:\n","        df (DataFrame): The input DataFrame containing various columns with different data types.\n","        target_type (DataType, optional): The target data type to cast numeric columns to. Defaults to DoubleType().\n","\n","    Returns:\n","        DataFrame: The resulting DataFrame with numeric columns cast to the specified target type.\n","    '''\n","\n","    # Iterate over each field in the DataFrame schema\n","    for field in df.schema.fields:\n","        # Check if the field's data type is a numeric type (IntegerType, LongType, or DoubleType)\n","        if isinstance(field.dataType, (IntegerType, LongType, DoubleType)):\n","            # Cast the column to the target type\n","            df = df.withColumn(field.name, col(field.name).cast(target_type))\n","    \n","    # Return the DataFrame with updated column types\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:34:07.9506743Z","session_start_time":null,"execution_start_time":"2024-08-08T19:34:08.5731363Z","execution_finish_time":"2024-08-08T19:34:08.8733044Z","parent_msg_id":"dc572fa5-7fd1-4ade-a319-7a0dacd3bee1"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd84631b-7a1e-43f9-be96-d8681e9aa078"},{"cell_type":"code","source":["def insert_metadata(df):\n","    '''\n","    This function inserts metadata into the given DataFrame by adding a new column with the current timestamp \n","    formatted according to the Brazilian time zone.\n","\n","    The function performs the following steps:\n","    1. Defines the Brazilian time zone (\"America/Sao_Paulo\").\n","    2. Converts the current UTC timestamp to the Brazilian time zone.\n","    3. Formats the timestamp to a specific string format (\"yyyyMMddHHmmss\").\n","    4. Adds a new column \"INGESTION_DATE\" to the DataFrame with the formatted timestamp.\n","    5. Returns the DataFrame with the new metadata column.\n","\n","    Args:\n","        df (DataFrame): The input DataFrame to which the metadata column will be added.\n","\n","    Returns:\n","        DataFrame: The resulting DataFrame with an additional \"INGESTION_DATE\" column containing the formatted timestamp.\n","    '''\n","\n","    # Define the Brazilian time zone\n","    brazilian_time_zone = \"America/Sao_Paulo\"\n","    \n","    # Convert the current UTC timestamp to the Brazilian time zone\n","    brazilian_timestamp = from_utc_timestamp(current_timestamp(), brazilian_time_zone)\n","    \n","    # Format the timestamp as \"yyyyMMddHHmmss\"\n","    formatted_timestamp = date_format(brazilian_timestamp, \"yyyyMMddHHmmss\")\n","    \n","    # Add a new column \"INGESTION_DATE\" to the DataFrame with the formatted timestamp\n","    df = df.withColumn(\"INGESTION_DATE\", formatted_timestamp)\n","    \n","    # Return the DataFrame with the added metadata column\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:34:13.310554Z","session_start_time":null,"execution_start_time":"2024-08-08T19:34:13.8962173Z","execution_finish_time":"2024-08-08T19:34:14.1810741Z","parent_msg_id":"ba5ebe86-5c16-486d-9d33-b6530363bc28"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d096609-605e-4733-a261-bf94dc589c44"},{"cell_type":"code","source":["def write_parquet_file(df, table_name):\n","    '''\n","    This function writes a Spark DataFrame to a Parquet file and saves it as a Delta table.\n","\n","    The function performs the following steps:\n","    1. Extracts the `INGESTION_DATE` value from the DataFrame and uses it to construct the file path.\n","    2. Organizes the file path into a specific directory structure based on the year, month, day, and time.\n","    3. Writes the DataFrame to the constructed file path as a Parquet file.\n","    4. Saves the DataFrame as a Delta table with schema overwrite enabled.\n","\n","    Args:\n","        df (DataFrame): The Spark DataFrame to be written.\n","        table_name (str): The name of the table to be used for constructing the file path and saving the Delta table.\n","\n","    Returns:\n","        None\n","    '''\n","\n","    # Extract the distinct 'INGESTION_DATE' from the DataFrame\n","    parquet_table_name = df.select('INGESTION_DATE').distinct().collect()[0][0]\n","    \n","    # Parse the 'INGESTION_DATE' into year, month, day, and time components\n","    year = parquet_table_name[0:4]\n","    month = parquet_table_name[4:6]\n","    day = parquet_table_name[6:8]\n","    time = parquet_table_name[8:]\n","    \n","    # Construct the file path using the parsed components and the table name\n","    file_path = f\"abfss://6870845f-6c5b-4f2b-a193-d7c7bac67d3b@onelake.dfs.fabric.microsoft.com/a139e00c-b185-4563-9a2d-ccae704b3457/Files/{table_name}/{year}/{month}/{day}/{time}\"\n","    \n","    # Write the DataFrame to the constructed file path as a Parquet file\n","    df.write.parquet(file_path)\n","    \n","    # Save the DataFrame as a Delta table with schema overwrite enabled\n","    df.write \\\n","        .option(\"overwriteSchema\", \"true\") \\\n","        .format(\"delta\") \\\n","        .mode(\"overwrite\") \\\n","        .saveAsTable(table_name, mode=\"overwrite\", ifNotExists=True)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:35:00.4891039Z","session_start_time":null,"execution_start_time":"2024-08-08T19:35:01.0624619Z","execution_finish_time":"2024-08-08T19:35:01.3912498Z","parent_msg_id":"a60713e4-bd46-4c82-9ec5-7f650b10bbca"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44087e62-24a3-4bd3-8adc-6cc4225edc52"},{"cell_type":"code","source":["table_names_dag = ''\n","table_names_dag = \"['FPG010', 'FQ4010', 'STL010', 'SCP010']\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:35:03.3760435Z","session_start_time":null,"execution_start_time":"2024-08-08T19:35:03.8948733Z","execution_finish_time":"2024-08-08T19:35:04.151963Z","parent_msg_id":"190679ea-ba7d-4831-a990-3a7c323ecaed"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"editable":true,"run_control":{"frozen":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f71385a0-b07c-4ce2-a74c-5f61aeac681c"},{"cell_type":"code","source":["print(table_names_dag)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:35:06.3988111Z","session_start_time":null,"execution_start_time":"2024-08-08T19:35:06.8879956Z","execution_finish_time":"2024-08-08T19:35:07.1548868Z","parent_msg_id":"269e0489-d5ea-434e-9dcd-94f924c203f0"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['FPG010', 'FQ4010', 'STL010', 'SCP010']\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a86dcd5b-9157-40d3-8451-091804f39592"},{"cell_type":"code","source":["table_names = table_names_dag[1:-1].split(',')\n","table_names"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:35:10.2775733Z","session_start_time":null,"execution_start_time":"2024-08-08T19:35:10.9031266Z","execution_finish_time":"2024-08-08T19:35:11.2244357Z","parent_msg_id":"b09c9fed-073c-45d2-ada2-526501b61854"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"[\"'FPG010'\", \" 'FQ4010'\", \" 'STL010'\", \" 'SCP010'\"]"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17a8e617-f8d2-4d43-91bc-3d43b170b737"},{"cell_type":"code","source":["def generate_table_schema(table_name):\n","    '''\n","    This function retrieves the schema of a specified table from a database.\n","\n","    The function performs the following steps:\n","    1. Executes an SQL query to retrieve the schema information of the specified table from the `bronze_protheus_raw.tableschema` table.\n","    2. Returns the resulting schema as a Spark DataFrame.\n","\n","    Args:\n","        table_name (str): The name of the table for which to retrieve the schema.\n","\n","    Returns:\n","        DataFrame: A Spark DataFrame containing the schema information of the specified table.\n","    '''\n","\n","    # Execute SQL query to retrieve the schema of the specified table\n","    tables_schema = spark.sql(f\"SELECT * FROM bronze_protheus_raw.tableschema WHERE TABLE_NAME='{table_name}'\")\n","    \n","    # Return the resulting schema as a Spark DataFrame\n","    return tables_schema\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:37:31.9925801Z","session_start_time":null,"execution_start_time":"2024-08-08T19:37:32.5178387Z","execution_finish_time":"2024-08-08T19:37:32.7921546Z","parent_msg_id":"e346a9b6-1889-4fa1-9973-8c5a3459c56b"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b17189e4-4ecd-4f39-b14d-793171377089"},{"cell_type":"code","source":["# Types Documentation https://spark.apache.org/docs/latest/sql-ref-datatypes.html#:~:text=Spark%20SQL%20and%20DataFrames%20support%20the%20following%20data,6%20Interval%20types%20...%207%20Complex%20types%20\n","\n","def get_mapped_lists(tables_schema):\n","    '''\n","    This function maps SQL column names and data types to PySpark data types and returns various lists used for query generation and schema creation.\n","\n","    The function performs the following steps:\n","    1. Collects the `COLUMN_NAME` and `DATA_TYPE` from the provided table schema DataFrame.\n","    2. Maps the SQL data types to PySpark data types using a predefined mapping.\n","    3. Generates a list of column names.\n","    4. Removes specific columns from the list of column names (if required).\n","    5. Generates a comma-separated string of the final list of column names.\n","\n","    Args:\n","        tables_schema (DataFrame): The Spark DataFrame containing the schema information of a table.\n","\n","    Returns:\n","        tuple: A tuple containing the following elements:\n","            - mapped_columns (list): A list of tuples where each tuple contains a column name and its corresponding PySpark data type.\n","            - first_elements_list_final (list): A list of column names after removing any specified columns.\n","            - columns_string (str): A comma-separated string of the final list of column names.\n","    '''\n","\n","    # Collect 'COLUMN_NAME' and 'DATA_TYPE' from the provided schema DataFrame\n","    aux_list = tables_schema.select('COLUMN_NAME', 'DATA_TYPE').collect()\n","    \n","    # Convert the collected rows into a list of tuples\n","    tuples_list = [tuple(row) for row in aux_list]\n","\n","    # Define a mapping from SQL data types to PySpark data types\n","    data_type_map = {\n","        'varchar': StringType(),\n","        'varbinary': StringType(),\n","        'float': DoubleType(),\n","        'bigint': LongType(),\n","        'datetime': TimestampType()\n","    }\n","    \n","    # Map SQL column names and data types to PySpark data types\n","    mapped_columns = [(col[0], data_type_map[col[1]]) for col in tuples_list]\n","\n","    # Generate a list of the first elements (column names) from the tuples\n","    first_elements_list = [item[0] for item in tuples_list]\n","    \n","    # Remove specific columns from the list (e.g., 'F1_ZJSONIN')\n","    columns_to_remove = ['F1_ZJSONIN']\n","    first_elements_list_final = list(set(first_elements_list) - set(columns_to_remove))\n","\n","    # Create a comma-separated string of the final list of column names\n","    columns_string = \", \".join(first_elements_list_final)\n","    \n","    return mapped_columns, first_elements_list_final, columns_string\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:37:29.3216373Z","session_start_time":null,"execution_start_time":"2024-08-08T19:37:29.8836119Z","execution_finish_time":"2024-08-08T19:37:30.2224497Z","parent_msg_id":"052e81d1-8717-429d-a5d2-d0181b2c6976"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"07f41795-67f7-421a-a65e-1de62a92a0b9"},{"cell_type":"code","source":["def create_struct_type(mapped_columns):\n","    '''\n","    This function dynamically creates a PySpark `StructType` schema based on the provided mapped columns.\n","\n","    The function performs the following steps:\n","    1. Iterates over the list of mapped columns to create a list of `StructField` objects.\n","    2. Combines the `StructField` objects into a `StructType` schema.\n","    3. Returns both the list of `StructField` objects and the constructed `StructType` schema.\n","\n","    Args:\n","        mapped_columns (list): A list of tuples where each tuple contains a column name and its corresponding PySpark data type.\n","\n","    Returns:\n","        tuple: A tuple containing the following elements:\n","            - fields (list): A list of `StructField` objects representing the columns and their data types.\n","            - schema (StructType): A `StructType` object representing the schema of the DataFrame.\n","    '''\n","\n","    # Create a list of StructField objects from the mapped columns\n","    fields = [StructField(name, dtype, True) for name, dtype in mapped_columns]\n","    \n","    # Combine the StructField objects into a StructType schema\n","    schema = StructType(fields)\n","    \n","    # Return the list of StructField objects and the constructed StructType schema\n","    return fields, schema\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:38:28.0601623Z","session_start_time":null,"execution_start_time":"2024-08-08T19:38:28.7579014Z","execution_finish_time":"2024-08-08T19:38:29.0413807Z","parent_msg_id":"6f7dc758-2064-4204-a92f-70d82194f080"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4d91a4e-f6e3-40e7-8d15-b6c38e704dd1"},{"cell_type":"code","source":["# Get secrets from Azure Key Vault\n","ssh_host = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','ssh-host')\n","ssh_port = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','ssh-port')\n","ssh_username = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','ssh-username')\n","ssh_password = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','ssh-password')\n","sql_host = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','sql-host')\n","sql_port = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','sql-port')\n","sql_username = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','sql-username')\n","sql_password = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','sql-password')\n","sql_database = mssparkutils.credentials.getSecret('https://dataengineervault.vault.azure.net/','sql-database')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:40:53.7890457Z","session_start_time":null,"execution_start_time":"2024-08-08T19:40:54.3256112Z","execution_finish_time":"2024-08-08T19:40:56.8434469Z","parent_msg_id":"b3e0ccc7-568e-4532-8944-7b89921eab6d"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1197290d-b6ce-4221-9a04-b5e35ce11c33"},{"cell_type":"code","source":["def generate_query_string(first_elements_list, table):\n","    '''\n","    This function generates an SQL SELECT query string for a specified table and list of columns.\n","\n","    The function performs the following steps:\n","    1. Joins the list of column names into a comma-separated string.\n","    2. Forms the SELECT query using the provided table name and columns string.\n","\n","    Args:\n","        first_elements_list (list): A list of column names to include in the SELECT query.\n","        table (str): The name of the table from which to select data.\n","\n","    Returns:\n","        str: An SQL SELECT query string that can be executed against the specified table.\n","    '''\n","\n","    # Join the list of column names into a comma-separated string\n","    columns_string = \", \".join(first_elements_list)\n","\n","    # Forming the SELECT query using the table name and columns string\n","    query_string = f\"SELECT {columns_string} FROM {table}\"\n","    \n","    return query_string\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:43:21.1850909Z","session_start_time":null,"execution_start_time":"2024-08-08T19:43:21.8054876Z","execution_finish_time":"2024-08-08T19:43:22.0726772Z","parent_msg_id":"d6dd3d4b-ef95-487b-aee8-772be94347f1"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"06592812-d314-443c-89b5-eb0c90e280d2"},{"cell_type":"code","source":["\n","# Create SSH tunnel\n","with sshtunnel.SSHTunnelForwarder(\n","    (ssh_host, int(ssh_port)),\n","    ssh_username=ssh_username,\n","    ssh_password=ssh_password,\n","    remote_bind_address=(sql_host, int(sql_port))\n",") as tunnel:\n","    server = tunnel.local_bind_address[0]\n","    \n","    port = tunnel.local_bind_address[1]\n","    print(f'{server}:{port}')\n","    tunnel.skip_tunnel_checkup = False\n","    tunnel.start()\n","    tunnel.check_tunnels()\n","    print(tunnel.tunnel_is_up, flush=True)\n","\n","    # Initialize Spark session with specific configurations\n","    spark = SparkSession.builder.appName(\"JDBCExample\") \\\n","        .config(\"spark.sql.parquet.vorder.enabled\", \"true\") \\\n","        .config(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") \\\n","        .getOrCreate()\n","\n","    for table in table_names:\n","        table = table.replace(' ','').replace('\\'', '')\n","        print(table)\n","        tables_schema = generate_table_schema(table)\n","\n","        mapped_columns, first_elements_list, columns_string = get_mapped_lists(tables_schema)\n","        #fields, schema = create_struct_type(mapped_columns)\n","        errors = []\n","        successes = []\n","\n","        query_string = generate_query_string(first_elements_list,table)\n","        print(query_string)\n","        df = spark.read.format(\"jdbc\") \\\n","            .option(\"url\", f\"jdbc:sqlserver://;serverName={socket.gethostname()};port={port};databaseName={sql_database};encrypt=true;trustServerCertificate=true\") \\\n","            .option(\"query\", query_string) \\\n","            .option(\"user\", sql_username) \\\n","            .option(\"password\", sql_password) \\\n","            .load()\n","        \n","\n","        df = cast_numeric_columns_to_common_type(df)\n","        df = insert_metadata(df)\n","        write_parquet_file(df,table)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T19:43:23.9783096Z","session_start_time":null,"execution_start_time":"2024-08-08T19:43:24.4365021Z","execution_finish_time":"2024-08-08T19:46:45.7269852Z","parent_msg_id":"6dfc914a-d5c9-41dd-bcc5-b5a3752e7011"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0.0.0.0:42701\n{('0.0.0.0', 42701): True}\nFPG010\nSELECT FPG_VLUNIT, FPG_ZPLACA, FPG_DTENT, FPG_PROJET, FPG_OK, FPG_NATURE, FPG_FILIAL, FPG_VALOR, FPG_STATUS, FPG_PVNUM, FPG_DOCORI, FPG_DESPES, FPG_TAXAV, FPG_ZMULTR, FPG_TAXAP, FPG_TIPO, FPG_QUANT, FPG_DESCRI, FPG_VALTOT, FPG_OBRA, R_E_C_D_E_L_, FPG_NRAS, FPG_RECORI, FPG_CUSTO, FPG_COBRAT, FPG_JUNTO, FPG_COBRA, FPG_PVITEM, FPG_PRODUT, FPG_DTPAG, FPG_SEQ, FPG_USERGI, FPG_CODDES, FPG_USERGA, FPG_ZOBS, R_E_C_N_O_, D_E_L_E_T_ FROM FPG010\nFQ4010\nSELECT FQ4_NOME, FQ4_ZSUBST, FQ4_NOMTRA, FQ4_ZDESSU, FQ4_STSOLD, FQ4_SERVIC, FQ4_CODBEM, FQ4_OS, FQ4_DTFIM, FQ4_CODFAM, FQ4_PRELIB, FQ4_TPSERV, FQ4_SERREM, FQ4_DOCUME, FQ4_FABRIC, FQ4_CENTRA, FQ4_SERIE, FQ4_EST, FQ4_MUNIC, FQ4_DESTAT, FQ4_LOJCLI, R_E_C_D_E_L_, FQ4_POSCON, FQ4_STATUS, FQ4_AS, FQ4_SUBLOC, FQ4_FILIAL, FQ4_TIPMOD, FQ4_DTINI, FQ4_ZANAMU, FQ4_PREDES, FQ4_ZDTTIM, FQ4_LOG, FQ4_NFREM, FQ4_CODMUN, FQ4_NOMCLI, FQ4_SEQ, FQ4_ZUSER, FQ4_PROJET, R_E_C_N_O_, D_E_L_E_T_, FQ4_CODCLI, FQ4_OBRA FROM FQ4010\nSTL010\nSELECT TL_HREXTRA, TL_PERMDOE, TL_ITEMAE, TL_TAREFA, TL_DOC, TL_ETAPA, TL_SEQUENC, TL_SERIE, TL_XMSGAUX, TL_XCODFIN, TL_ITEMSC, TL_LOCAL, TL_ZITEMPC, TL_XVLRUNI, TL_USACALE, TL_NOTFIS, TL_QUANREC, TL_ITEMSA, TL_XCUSEX, TL_HOINICI, TL_ORDEM, TL_XCOBRA, TL_HOFIM, TL_XVLRTOT, TL_ZNUMPC, TL_CODIGO, TL_AS, TL_PCTHREX, TL_LOTECTL, R_E_C_D_E_L_, TL_SEQTARE, TL_XVLRATO, TL_ZTPINS, TL_ORIGNFE, TL_ZCODFAB, TL_CUSTO, TL_QUANTID, TL_XPERCVU, TL_FILIAL, TL_SEQRMAN, TL_XJUSTIF, TL_XDESFIN, TL_ZFORNEC, TL_TIPOHOR, R_E_C_N_O_, TL_XTROCAP, TL_ITEM, TL_SEQRELA, TL_NUMAE, TL_FORNEC, TL_NUMLOTE, TL_POSCON2, TL_NUMSC, TL_NUMSA, TL_OBSERVA, TL_SDOC, TL_DESTINO, TL_SEQMAN, TL_UNIDADE, TL_LOCAPLI, TL_POSCONT, TL_CODBEM, TL_NUMOP, TL_DTFIM, TL_LOJA, TL_XVLRSET, TL_XDESCSE, TL_NUMSERI, TL_XTIPOPR, TL_DTINICI, TL_ZLOJFOR, TL_XVLRAUN, TL_XULTTRO, TL_SEQUEOP, TL_ITEMOP, TL_SERVICO, TL_NUMSEQ, TL_PLANO, TL_LOCALIZ, TL_GARANTI, D_E_L_E_T_, TL_TIPOREG, TL_DTVALID, TL_REPFIM, TL_CODAEN FROM STL010\nSCP010\nSELECT CP_MEDIDA, CP_USER, CP_NUMSC, CP_CC, CP_SULCMA, CP_STATSA, CP_EMISSAO, CP_OK, CP_SEQRC, CP_CLVL, CP_ITEM, CP_OBS, CP_RATEIO, CP_SALBLQ, CP_ITSC, CP_USERLGA, CP_SEGUM, CP_TRT, CP_DESCRI, CP_LOTE, CP_DATPRF, CP_STATUS, CP_LOCAL, CP_CONTA, CP_PROJETO, CP_USERLGI, CP_TIPMOD, CP_NUMOS, CP_ZCTAORC, CP_ITEMCTA, R_E_C_D_E_L_, CP_VUNIT, CP_UM, CP_NUM, CP_FILIAL, CP_PREREQU, CP_QUJE, CP_NRBPIMS, CP_QUANT, CP_CODSOLI, CP_SOLICIT, CP_PRODUTO, CP_CONSEST, CP_QTSEGUM, D_E_L_E_T_, R_E_C_N_O_, CP_SULCMI, CP_OP FROM SCP010\n"]}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"378dae1a-ba3b-4ab7-89e0-2434f7ba2190"},{"cell_type":"code","source":["# Create SSH tunnel\n","with sshtunnel.SSHTunnelForwarder(\n","    (ssh_host, int(ssh_port)),\n","    ssh_username=ssh_username,\n","    ssh_password=ssh_password,\n","    remote_bind_address=(sql_host, int(sql_port))\n",") as tunnel:\n","    '''\n","    Establishes an SSH tunnel to a remote SQL server, forwarding local connections to the remote server.\n","\n","    This block:\n","    1. Creates and starts an SSH tunnel to forward local connections to the SQL server.\n","    2. Prints the local server and port information for verification.\n","    3. Ensures the tunnel is active and properly set up.\n","\n","    Attributes:\n","        ssh_host (str): The SSH server hostname.\n","        ssh_port (int): The SSH server port.\n","        ssh_username (str): The username for SSH authentication.\n","        ssh_password (str): The password for SSH authentication.\n","        sql_host (str): The SQL server hostname.\n","        sql_port (int): The SQL server port.\n","    '''\n","\n","    server = tunnel.local_bind_address[0]\n","    port = tunnel.local_bind_address[1]\n","    print(f'{server}:{port}')\n","    tunnel.skip_tunnel_checkup = False\n","    tunnel.start()\n","    tunnel.check_tunnels()\n","    print(tunnel.tunnel_is_up, flush=True)\n","\n","    # Initialize Spark session with specific configurations\n","    spark = SparkSession.builder.appName(\"JDBCExample\") \\\n","        .config(\"spark.sql.parquet.vorder.enabled\", \"true\") \\\n","        .config(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") \\\n","        .getOrCreate()\n","\n","    '''\n","    Initializes a Spark session with configurations for parquet and delta optimizations.\n","\n","    Attributes:\n","        spark (SparkSession): The Spark session object.\n","    '''\n","\n","    for table in table_names:\n","        '''\n","        Processes each table in the provided list of table names by:\n","        1. Cleaning up the table name.\n","        2. Retrieving the schema of the table.\n","        3. Mapping columns and data types.\n","        4. Generating the SQL query string.\n","        5. Loading data from the SQL server into a DataFrame.\n","        6. Casting numeric columns to a common type.\n","        7. Inserting metadata.\n","        8. Writing the DataFrame to a Parquet file and a Delta table.\n","\n","        Attributes:\n","            table_names (list): A list of table names to process.\n","        '''\n","\n","        table = table.replace(' ', '').replace('\\'', '')\n","        print(table)\n","        tables_schema = generate_table_schema(table)\n","\n","        mapped_columns, first_elements_list, columns_string = get_mapped_lists(tables_schema)\n","        #fields, schema = create_struct_type(mapped_columns)\n","        errors = []\n","        successes = []\n","\n","        query_string = generate_query_string(first_elements_list, table)\n","        print(query_string)\n","        df = spark.read.format(\"jdbc\") \\\n","            .option(\"url\", f\"jdbc:sqlserver://;serverName={socket.gethostname()};port={port};databaseName={sql_database};encrypt=true;trustServerCertificate=true\") \\\n","            .option(\"query\", query_string) \\\n","            .option(\"user\", sql_username) \\\n","            .option(\"password\", sql_password) \\\n","            .load()\n","        \n","        # Cast numeric columns to a common data type\n","        df = cast_numeric_columns_to_common_type(df)\n","        # Insert ingestion metadata into the DataFrame\n","        df = insert_metadata(df)\n","        # Write DataFrame to both Parquet file and Delta table\n","        write_parquet_file(df, table)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16b6ae07-d25d-4577-938e-44b5bd7c5984"},{"cell_type":"code","source":["\"\"\"\n","This script establishes an SSH tunnel to securely connect to a remote SQL Server database, \n","then initializes a Spark session to extract data from specified tables, process it, \n","and write the resulting DataFrame as Parquet files.\n"," \n","The script performs the following steps:\n","1. Establishes an SSH tunnel using the `sshtunnel` library.\n","2. Initializes a Spark session with specific configurations for Parquet and Delta Lake optimization.\n","3. Iterates through a list of table names to:\n","    - Clean the table name.\n","    - Generate the schema and query string for the table.\n","    - Read the table data into a Spark DataFrame using JDBC.\n","    - Process the DataFrame (e.g., casting numeric columns, inserting metadata).\n","    - Write the DataFrame as a Parquet file.\n","\"\"\"\n"," \n","# Create SSH tunnel\n","with sshtunnel.SSHTunnelForwarder(\n","    (ssh_host, int(ssh_port)),\n","    ssh_username=ssh_username,\n","    ssh_password=ssh_password,\n","    remote_bind_address=(sql_host, int(sql_port))\n",") as tunnel:\n","    server = tunnel.local_bind_address[0]  # Get local server address\n","    port = tunnel.local_bind_address[1]    # Get local port address\n","    print(f'{server}:{port}')  # Output the server and port to confirm connection\n"," \n","    tunnel.skip_tunnel_checkup = False  # Ensure the tunnel performs a checkup\n","    tunnel.start()  # Start the SSH tunnel\n","    tunnel.check_tunnels()  # Check the status of the tunnel\n","    print(tunnel.tunnel_is_up, flush=True)  # Print tunnel status (True if up)\n"," \n","    # Initialize Spark session with specific configurations\n","    spark = SparkSession.builder.appName(\"JDBCExample\") \\\n","        .config(\"spark.sql.parquet.vorder.enabled\", \"true\") \\\n","        .config(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") \\\n","        .getOrCreate()\n"," \n","    # Iterate over the list of table names\n","    for table in table_names:\n","        table = table.replace(' ', '').replace('\\'', '')  # Clean the table name\n","        print(table)  # Print the cleaned table name\n"," \n","        tables_schema = generate_table_schema(table)  # Generate the schema for the table\n"," \n","        # Get the mapped columns, first elements list, and columns string\n","        mapped_columns, first_elements_list, columns_string = get_mapped_lists(tables_schema)\n"," \n","        # Initialize lists for errors and successes\n","        errors = []\n","        successes = []\n"," \n","        # Generate the SQL query string for the table\n","        query_string = generate_query_string(first_elements_list, table)\n","        print(query_string)  # Print the generated query string\n"," \n","        # Read data from the table into a Spark DataFrame using JDBC\n","        df = spark.read.format(\"jdbc\") \\\n","            .option(\"url\", f\"jdbc:sqlserver://;serverName={socket.gethostname()};port={port};databaseName={sql_database};encrypt=true;trustServerCertificate=true\") \\\n","            .option(\"query\", query_string) \\\n","            .option(\"user\", sql_username) \\\n","            .option(\"password\", sql_password) \\\n","            .load()\n"," \n","        # Process the DataFrame\n","        df = cast_numeric_columns_to_common_type(df)  # Cast numeric columns to a common type\n","        df = insert_metadata(df)  # Insert metadata into the DataFrame\n"," \n","        # Write the processed DataFrame as a Parquet file\n","        write_parquet_file(df, table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T20:12:54.5832466Z","session_start_time":null,"execution_start_time":"2024-08-08T20:12:55.1445954Z","execution_finish_time":"2024-08-08T20:16:07.6856744Z","parent_msg_id":"520e7c2a-78a8-4c76-bcec-75ae32385395"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 24, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0.0.0.0:38975\n{('0.0.0.0', 38975): True}\nFPG010\nSELECT FPG_VLUNIT, FPG_ZPLACA, FPG_DTENT, FPG_PROJET, FPG_OK, FPG_NATURE, FPG_FILIAL, FPG_VALOR, FPG_STATUS, FPG_PVNUM, FPG_DOCORI, FPG_DESPES, FPG_TAXAV, FPG_ZMULTR, FPG_TAXAP, FPG_TIPO, FPG_QUANT, FPG_DESCRI, FPG_VALTOT, FPG_OBRA, R_E_C_D_E_L_, FPG_NRAS, FPG_RECORI, FPG_CUSTO, FPG_COBRAT, FPG_JUNTO, FPG_COBRA, FPG_PVITEM, FPG_PRODUT, FPG_DTPAG, FPG_SEQ, FPG_USERGI, FPG_CODDES, FPG_USERGA, FPG_ZOBS, R_E_C_N_O_, D_E_L_E_T_ FROM FPG010\nFQ4010\nSELECT FQ4_NOME, FQ4_ZSUBST, FQ4_NOMTRA, FQ4_ZDESSU, FQ4_STSOLD, FQ4_SERVIC, FQ4_CODBEM, FQ4_OS, FQ4_DTFIM, FQ4_CODFAM, FQ4_PRELIB, FQ4_TPSERV, FQ4_SERREM, FQ4_DOCUME, FQ4_FABRIC, FQ4_CENTRA, FQ4_SERIE, FQ4_EST, FQ4_MUNIC, FQ4_DESTAT, FQ4_LOJCLI, R_E_C_D_E_L_, FQ4_POSCON, FQ4_STATUS, FQ4_AS, FQ4_SUBLOC, FQ4_FILIAL, FQ4_TIPMOD, FQ4_DTINI, FQ4_ZANAMU, FQ4_PREDES, FQ4_ZDTTIM, FQ4_LOG, FQ4_NFREM, FQ4_CODMUN, FQ4_NOMCLI, FQ4_SEQ, FQ4_ZUSER, FQ4_PROJET, R_E_C_N_O_, D_E_L_E_T_, FQ4_CODCLI, FQ4_OBRA FROM FQ4010\nSTL010\nSELECT TL_HREXTRA, TL_PERMDOE, TL_ITEMAE, TL_TAREFA, TL_DOC, TL_ETAPA, TL_SEQUENC, TL_SERIE, TL_XMSGAUX, TL_XCODFIN, TL_ITEMSC, TL_LOCAL, TL_ZITEMPC, TL_XVLRUNI, TL_USACALE, TL_NOTFIS, TL_QUANREC, TL_ITEMSA, TL_XCUSEX, TL_HOINICI, TL_ORDEM, TL_XCOBRA, TL_HOFIM, TL_XVLRTOT, TL_ZNUMPC, TL_CODIGO, TL_AS, TL_PCTHREX, TL_LOTECTL, R_E_C_D_E_L_, TL_SEQTARE, TL_XVLRATO, TL_ZTPINS, TL_ORIGNFE, TL_ZCODFAB, TL_CUSTO, TL_QUANTID, TL_XPERCVU, TL_FILIAL, TL_SEQRMAN, TL_XJUSTIF, TL_XDESFIN, TL_ZFORNEC, TL_TIPOHOR, R_E_C_N_O_, TL_XTROCAP, TL_ITEM, TL_SEQRELA, TL_NUMAE, TL_FORNEC, TL_NUMLOTE, TL_POSCON2, TL_NUMSC, TL_NUMSA, TL_OBSERVA, TL_SDOC, TL_DESTINO, TL_SEQMAN, TL_UNIDADE, TL_LOCAPLI, TL_POSCONT, TL_CODBEM, TL_NUMOP, TL_DTFIM, TL_LOJA, TL_XVLRSET, TL_XDESCSE, TL_NUMSERI, TL_XTIPOPR, TL_DTINICI, TL_ZLOJFOR, TL_XVLRAUN, TL_XULTTRO, TL_SEQUEOP, TL_ITEMOP, TL_SERVICO, TL_NUMSEQ, TL_PLANO, TL_LOCALIZ, TL_GARANTI, D_E_L_E_T_, TL_TIPOREG, TL_DTVALID, TL_REPFIM, TL_CODAEN FROM STL010\nSCP010\nSELECT CP_MEDIDA, CP_USER, CP_NUMSC, CP_CC, CP_SULCMA, CP_STATSA, CP_EMISSAO, CP_OK, CP_SEQRC, CP_CLVL, CP_ITEM, CP_OBS, CP_RATEIO, CP_SALBLQ, CP_ITSC, CP_USERLGA, CP_SEGUM, CP_TRT, CP_DESCRI, CP_LOTE, CP_DATPRF, CP_STATUS, CP_LOCAL, CP_CONTA, CP_PROJETO, CP_USERLGI, CP_TIPMOD, CP_NUMOS, CP_ZCTAORC, CP_ITEMCTA, R_E_C_D_E_L_, CP_VUNIT, CP_UM, CP_NUM, CP_FILIAL, CP_PREREQU, CP_QUJE, CP_NRBPIMS, CP_QUANT, CP_CODSOLI, CP_SOLICIT, CP_PRODUTO, CP_CONSEST, CP_QTSEGUM, D_E_L_E_T_, R_E_C_N_O_, CP_SULCMI, CP_OP FROM SCP010\n"]}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"072d7e06-01aa-4982-a97e-fd2d7203e69c"},{"cell_type":"code","source":["for table in table_names:\n","    table = table.replace(' ','').replace('\\'', '')\n","    print(table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"025c3015-e649-4543-86ef-80d9a6d021e9","normalized_state":"finished","queued_time":"2024-08-08T20:06:13.9432017Z","session_start_time":null,"execution_start_time":"2024-08-08T20:06:14.6629222Z","execution_finish_time":"2024-08-08T20:06:15.1374101Z","parent_msg_id":"6f9492a9-dd38-4c76-856e-c770962d1d6d"},"text/plain":"StatementMeta(, 025c3015-e649-4543-86ef-80d9a6d021e9, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["FPG010\nFQ4010\nSTL010\nSCP010\n"]}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0fd3d1cb-687d-40f9-9df0-f8f1af2a5f50"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"environment":{"environmentId":"f7759f68-3434-4055-99de-7f7ebbc9e558","workspaceId":"6870845f-6c5b-4f2b-a193-d7c7bac67d3b"},"lakehouse":{"default_lakehouse":"a139e00c-b185-4563-9a2d-ccae704b3457","default_lakehouse_name":"bronze_protheus_raw","default_lakehouse_workspace_id":"6870845f-6c5b-4f2b-a193-d7c7bac67d3b"}}},"nbformat":4,"nbformat_minor":5}